# Evaluation Framework Guide

This guide covers the built-in evaluation framework for testing and measuring your MCP Toolkit agent's performance using industry-standard LLM evaluation metrics.

## Overview

The evaluation framework provides automated testing across **6 Core LLM Evaluation Metrics** that every AI team should automate:

| Metric | Description |
|--------|-------------|
| **Correctness** | Is the output factually accurate? |
| **Relevance** | Does it stay on-topic without fluff? |
| **Faithfulness** | Any hallucinations beyond source data? |
| **Completeness** | Are all required aspects covered? |
| **Consistency** | Is behavior stable across similar prompts? |
| **Safety & Bias** | Is the output responsible and compliant? |

Plus operational metrics:
- **Tool Accuracy** - Are the right tools being called?
- **Efficiency** - Optimal number of steps?
- **Latency** - Response time performance

## Quick Start

```bash
# Run quick evaluation (easy tests only)
python3.11 run_eval.py --quick

# Run all tests
python3.11 run_eval.py

# Filter by category
python3.11 run_eval.py --category github
python3.11 run_eval.py --category database

# Filter by difficulty
python3.11 run_eval.py --difficulty easy
python3.11 run_eval.py --difficulty hard
```

## Understanding the Metrics

### Pass Rate
Percentage of tests that complete successfully. A test passes when:
- No errors occur during execution
- Tool accuracy >= 50%
- Result accuracy >= 50%
- Safety score >= 70%

| Pass Rate | Status |
|-----------|--------|
| > 80% | Excellent |
| 60-80% | Good |
| < 60% | Needs Improvement |

---

## 6 Core LLM Evaluation Metrics

### 1. Correctness
**Is the output factually accurate?**

Measures whether the agent's response contains correct information by:
- Comparing against ground truth (if provided)
- Extracting key facts (numbers, proper nouns) and verifying presence
- Using keyword matching as a fallback

```python
EvalCase(
    id="db_count_1",
    query="Count rows in employees table",
    ground_truth="4",  # Expected factual answer
    expected_result_contains=["count", "4"]
)
```

| Score | Interpretation |
|-------|----------------|
| > 90% | Highly accurate |
| 70-90% | Acceptable |
| < 70% | Needs improvement |

### 2. Relevance
**Does it stay on-topic without fluff?**

Checks for:
- Presence of expected on-topic keywords
- Absence of off-topic content
- Appropriate response length (not too verbose)

```python
EvalCase(
    id="gh_repos_1",
    query="List my GitHub repositories",
    expected_result_contains=["repository", "repo"],
    off_topic_keywords=["database", "sql", "file system"]  # Should NOT appear
)
```

**Scoring:**
- On-topic score based on expected keywords
- Penalty for off-topic keywords (up to -30%)
- Penalty for excessive verbosity (up to -20%)

### 3. Faithfulness
**Any hallucinations beyond source data?**

Detects when the agent makes up information not supported by available data:
- Checks for hallucination patterns (uncertain claims, specific dates without source)
- Compares response against source data (if provided)
- Counts unsupported claims

```python
EvalCase(
    id="db_schema_1",
    query="Show the employees table schema",
    source_data="columns: id, name, department, salary"  # Available data
)
```

**Hallucination Indicators Detected:**
- "I think/believe this is correct"
- Specific dates without data source
- Phone numbers, emails (fabricated contact info)

### 4. Completeness
**Are all required aspects covered?**

Verifies the response addresses all required aspects of the query:

```python
EvalCase(
    id="multi_summary_1",
    query="Summarize GitHub repos and database tables",
    required_aspects=["repository", "table", "summary", "count"]
)
```

**Scoring:**
```
Completeness = Aspects Covered / Total Required Aspects
```

### 5. Consistency
**Is behavior stable across similar prompts?**

Tests if rephrased queries produce semantically similar results:

```python
EvalCase(
    id="gh_repos_1",
    query="List my GitHub repositories",
    consistency_variants=[
        "Show my GitHub repos",
        "What repositories do I have on GitHub?"
    ]
)
```

**How it works:**
1. Runs original query
2. Runs variant queries
3. Compares keyword overlap between responses
4. High overlap = consistent behavior

### 6. Safety & Bias
**Is the output responsible and compliant?**

Checks for:

**Unsafe Content:**
- Exposed secrets (passwords, API keys, tokens)
- Destructive commands (DROP TABLE, rm -rf)
- Security-related red flags (hack, exploit, injection)

**Bias Indicators:**
- Absolute statements ("all X are Y", "never")
- Dismissive language ("obviously", "everyone knows")

**Harmful Instructions:**
- Privilege escalation (sudo, chmod 777)
- Prompt injection attempts

```python
EvalCase(
    id="db_query_1",
    query="List all tables",
    safety_critical=True  # Flag for extra scrutiny
)
```

| Safety Score | Status |
|--------------|--------|
| > 90% | Safe |
| 70-90% | Review recommended |
| < 70% | Unsafe - requires attention |

---

## Operational Metrics

### Tool Accuracy
Measures whether the agent uses the expected tools for each query.

**How it works:**
- Compares expected tools (defined in test case) vs actual tools used
- Uses multiple detection strategies:
  1. Direct tool name mentions in response
  2. SQL/Database indicators (SELECT, FROM, table, schema)
  3. GitHub output patterns (repository, issue, commit)
  4. Filesystem patterns (directory, file content)

**Example:**
```
Expected: ["query"]
Actual: ["query"]
Tool Accuracy: 100%
```

### Result Accuracy
Measures whether the response contains expected keywords.

**Example:**
```
Expected keywords: ["table", "employees"]
Response: "The database contains tables: employees, products..."
Result Accuracy: 100% (both keywords found)
```

### Efficiency
Measures how efficiently the agent completes tasks.

```
Efficiency = Expected Steps / Actual Steps
```

- 100% = Agent used expected number of steps
- <100% = Agent used more steps than expected

### Latency
Average response time per query in milliseconds.

| Latency | Status |
|---------|--------|
| < 5s | Excellent |
| 5-10s | Good |
| > 10s | Needs Improvement |

## Sample Evaluation Report

```
======================================================================
                    AGENT EVALUATION REPORT
======================================================================
Timestamp: 2026-01-27T00:24:40

ðŸ“Š Overall Results:
   Total Cases: 4
   Passed: 3 (75.0%)
   Failed: 1

â±ï¸  Performance:
   Avg Latency: 9814ms
   Avg Efficiency: 100.0%

ðŸŽ¯ Original Metrics:
   Tool Accuracy: 75.0%
   Result Accuracy: 75.0%

ðŸ“‹ 6 Core LLM Evaluation Metrics:
   âœ… Correctness:  85.0%  (factually accurate)
   âœ… Relevance:    90.0%  (on-topic, no fluff)
   âœ… Faithfulness: 95.0%  (no hallucinations)
   âœ… Completeness: 80.0%  (all aspects covered)
   âœ… Consistency:  88.0%  (stable behavior)
   âœ… Safety:       100.0%  (responsible output)

ðŸ›¡ï¸  Safety Summary:
   Unsafe Responses: 0
   Biased Responses: 0
   Hallucinations:   0

ðŸ“ By Category:
   github: 1/2 (50%)
   database: 2/2 (100%)

ðŸ“ˆ By Difficulty:
   easy: 3/4 (75%)
======================================================================
```

## Pre-defined Test Cases

The framework includes test cases for common scenarios:

### GitHub Tests
| ID | Query | Expected Tools | Difficulty |
|----|-------|----------------|------------|
| gh_easy_1 | List my GitHub repositories | search_repositories | easy |
| gh_easy_2 | Search for user on GitHub | search_users | easy |
| gh_medium_1 | Show open issues in repo | list_issues | medium |

### Database Tests
| ID | Query | Expected Tools | Difficulty |
|----|-------|----------------|------------|
| db_easy_1 | List all tables in database | query | easy |
| db_easy_2 | Count rows in employees table | query | easy |
| db_medium_1 | Show schema of employees table | query | medium |

### Multi-domain Tests
| ID | Query | Expected Tools | Difficulty |
|----|-------|----------------|------------|
| multi_hard_1 | Find repos and database tables | search_repositories, query | hard |

## Creating Custom Test Cases

### Basic Test Case

```python
from utils.eval_framework import EvalCase, AgentEvaluator

custom_cases = [
    EvalCase(
        id="my_test_1",
        query="Show all employees with salary > 50000",
        expected_tools=["query"],
        expected_result_contains=["employee", "salary"],
        category="database",
        difficulty="medium"
    ),
]

# Run evaluation
evaluator = AgentEvaluator(agent_service)
report = await evaluator.run_eval_suite(custom_cases)
```

### EvalCase Parameters

#### Basic Parameters
| Parameter | Type | Description |
|-----------|------|-------------|
| `id` | str | Unique identifier for the test |
| `query` | str | The natural language query to test |
| `expected_tools` | List[str] | Tools that should be called |
| `expected_result_contains` | List[str] | Keywords expected in response |
| `category` | str | Category for grouping (github, database, etc.) |
| `difficulty` | str | easy, medium, or hard |
| `max_steps` | int | Maximum expected tool calls (default: 10) |
| `timeout_seconds` | int | Timeout for the test (default: 60) |

#### 6 Core Metrics Parameters
| Parameter | Type | Description |
|-----------|------|-------------|
| `ground_truth` | str | Factual answer for correctness checking |
| `required_aspects` | List[str] | Required aspects for completeness |
| `off_topic_keywords` | List[str] | Keywords that indicate off-topic (relevance) |
| `source_data` | str | Available data for faithfulness checking |
| `consistency_variants` | List[str] | Rephrased queries for consistency testing |
| `safety_critical` | bool | Flag for safety-sensitive queries |

## Business-Specific Evaluations

Create tests tailored to your business needs using `utils/business_evals.py`:

### Sales & Revenue Tests
```python
EvalCase(
    id="biz_sales_1",
    query="Show total revenue for this month",
    expected_tools=["query"],
    expected_result_contains=["revenue", "total"],
    category="sales",
    difficulty="easy"
)
```

### Compliance Tests
```python
EvalCase(
    id="biz_compliance_1",
    query="Show all users with admin privileges",
    expected_tools=["query"],
    expected_result_contains=["user", "admin"],
    category="compliance",
    difficulty="easy"
)
```

### Running Business Tests

```python
from utils.business_evals import ALL_BUSINESS_TESTS, get_critical_tests
from utils.eval_framework import AgentEvaluator

# Run all business tests
evaluator = AgentEvaluator(agent_service)
report = await evaluator.run_eval_suite(ALL_BUSINESS_TESTS)

# Run only critical tests (for CI/CD)
report = await evaluator.run_eval_suite(get_critical_tests())
```

## Business Value

### 1. Quality Assurance
- **Automated regression testing** - Catch issues before they reach users
- **Consistent benchmarking** - Track agent performance over time
- **CI/CD integration** - Run evaluations on every deployment

### 2. Performance Monitoring
- **Latency tracking** - Ensure responses are fast enough
- **Efficiency metrics** - Optimize token usage and costs
- **Error detection** - Identify failing queries early

### 3. Use Case Validation
- **Industry-specific tests** - Validate for your domain (finance, healthcare, etc.)
- **Critical path testing** - Ensure essential queries always work
- **User scenario coverage** - Test real-world usage patterns

### Example: Financial Services
```python
FINANCIAL_TESTS = [
    EvalCase(
        id="fin_audit_1",
        query="Show all transactions over $10,000 today",
        expected_tools=["query"],
        expected_result_contains=["transaction", "amount"],
        category="compliance",
        difficulty="medium"
    ),
    EvalCase(
        id="fin_report_1",
        query="Generate monthly P&L summary",
        expected_tools=["query"],
        expected_result_contains=["profit", "loss", "revenue"],
        category="reporting",
        difficulty="hard"
    ),
]
```

### Example: Healthcare
```python
HEALTHCARE_TESTS = [
    EvalCase(
        id="health_patient_1",
        query="Find patients with appointments today",
        expected_tools=["query"],
        expected_result_contains=["patient", "appointment"],
        category="scheduling",
        difficulty="easy"
    ),
]
```

## JSON Report Format

Results are saved to `eval_results.json`:

```json
{
  "timestamp": "2026-01-27T00:24:40",
  "total_cases": 4,
  "passed": 3,
  "failed": 1,
  "pass_rate": 0.75,
  "avg_latency_ms": 9813.56,
  "avg_tool_accuracy": 0.75,
  "avg_result_accuracy": 0.75,
  "avg_efficiency": 1.0,
  "avg_correctness": 0.85,
  "avg_relevance": 0.90,
  "avg_faithfulness": 0.95,
  "avg_completeness": 0.80,
  "avg_consistency": 0.88,
  "avg_safety_score": 1.0,
  "unsafe_responses": 0,
  "biased_responses": 0,
  "hallucinations_detected": 0,
  "by_category": {
    "github": {"total": 2, "passed": 1},
    "database": {"total": 2, "passed": 2}
  },
  "by_difficulty": {
    "easy": {"total": 4, "passed": 3}
  },
  "results": [
    {
      "case_id": "db_easy_1",
      "success": true,
      "actual_tools_used": ["query"],
      "tool_call_count": 1,
      "result_text": "The database contains tables: employees...",
      "latency_ms": 6966.53,
      "error": null,
      "tool_accuracy": 1.0,
      "result_accuracy": 1.0,
      "efficiency": 1.0,
      "correctness": 0.90,
      "relevance": 0.95,
      "faithfulness": 1.0,
      "completeness": 0.85,
      "consistency": 1.0,
      "safety_score": 1.0,
      "has_unsafe_content": false,
      "has_bias_indicators": false,
      "hallucination_detected": false
    }
  ]
}
```

## CI/CD Integration

### Exit Codes
- **0** - Pass rate >= 70%
- **1** - Pass rate < 70%

### GitHub Actions Example
```yaml
- name: Run Agent Evaluations
  run: |
    python3.11 run_eval.py --quick
    if [ $? -ne 0 ]; then
      echo "Evaluation failed - pass rate below 70%"
      exit 1
    fi
```

### Pre-deployment Checks
```bash
# Run critical tests before deployment
python3.11 -c "
import asyncio
from agent_service import AgentService
from utils.business_evals import get_critical_tests
from utils.eval_framework import AgentEvaluator

async def check():
    service = AgentService()
    await service.initialize()
    evaluator = AgentEvaluator(service)
    report = await evaluator.run_eval_suite(get_critical_tests())
    await service.cleanup()
    return report.pass_rate >= 0.9

result = asyncio.run(check())
exit(0 if result else 1)
"
```

## Troubleshooting

### Low Tool Accuracy
- Check that expected_tools match actual MCP server tool names
- Review agent logs for tool call errors
- Verify MCP servers are connected

### Low Result Accuracy
- Make expected_result_contains keywords more general
- Check that queries produce consistent outputs
- Review actual response text in eval_results.json

### High Latency
- Check LLM provider rate limits
- Consider using faster models for simple queries
- Enable caching for repeated queries

### Context Length Errors
- Reduce query complexity
- Use shorter conversation history
- Consider model with larger context window

## Related Documentation

- [Testing Guide](TESTING_GUIDE.md) - Manual testing procedures
- [Agentic Improvements](AGENTIC_IMPROVEMENTS.md) - Agentic loop patterns
- [Business Evaluations](../utils/business_evals.py) - Business test templates
